{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsRMQyoEOlK"
      },
      "source": [
        "### Cài đặt thư viện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n19crguzETyi",
        "outputId": "30e04709-4b1e-4095-bf89-7e248a8c9808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.3.24 in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-openai==0.3.14 in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langgraph==0.3.33 in /usr/local/lib/python3.11/dist-packages (0.3.33)\n",
            "Requirement already satisfied: langchain-tavily==0.1.6 in /usr/local/lib/python3.11/dist-packages (0.1.6)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.24) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.14) (1.78.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.14) (0.9.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.3.33) (2.0.26)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.3.33) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.3.33) (0.1.69)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.3.33) (3.5.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily==0.1.6) (3.11.15)\n",
            "Requirement already satisfied: mypy<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily==0.1.6) (1.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.6) (1.20.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (4.13.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.33) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.24) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.24) (0.23.0)\n",
            "Requirement already satisfied: mypy_extensions>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mypy<2.0.0,>=1.15.0->langchain-tavily==0.1.6) (1.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai==0.3.14) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.24) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.24) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.24) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.24) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.24) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.24) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.14) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.33) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain==0.3.24) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain==0.3.24 langchain-openai==0.3.14 langgraph==0.3.33 langchain-tavily==0.1.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD7uKxJFEKMz"
      },
      "outputs": [],
      "source": [
        "# Install libs\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = ### Your-key\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Reasoning-Agent\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = ### Your-key\n",
        "os.environ[\"OPENAI_API_KEY\"] = ### Your-key\n",
        "\n",
        "from typing import Annotated,Sequence, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"The state of the agent.\"\"\"\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    number_of_steps: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_l25Ud_E2dw"
      },
      "source": [
        "### Định nghĩa các công cụ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BbB717SLENSx"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "@tool\n",
        "def triple(num: float)-> float:\n",
        "    \"\"\"\n",
        "    :param num: a number to triple\n",
        "    :return: the number tripled-> multiplied by 3\n",
        "    \"\"\"\n",
        "    return 3 * float(num)\n",
        "\n",
        "\n",
        "tools = [TavilySearch(max_results=1), triple]\n",
        "\n",
        "system_prompt = SystemMessage(\n",
        "    \"\"\"\n",
        "    You are a reasoning agent. Always think step-by-step.\n",
        "\n",
        "    Use the following format:\n",
        "\n",
        "    Thought: what you are thinking\n",
        "    Action: the action to take, e.g. ‘search‘, ‘calculate‘\n",
        "    Action Input: the input to the action\n",
        "    Observation: the result of the action\n",
        "\n",
        "    (Repeat Thought/Action/Observation if needed)\n",
        "\n",
        "    Final Answer: your answer to the user\n",
        "\n",
        "    Question: {input}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9BGNM3YFKuT"
      },
      "source": [
        "### Xây dựng các hàm thực hiện các tác vụ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "w9jGeQrPENQa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "# Define our tool node\n",
        "def tool_node(state: AgentState):\n",
        "    outputs = []\n",
        "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
        "        outputs.append(\n",
        "            ToolMessage(\n",
        "            content=json.dumps(tool_result),\n",
        "            name=tool_call[\"name\"],\n",
        "            tool_call_id=tool_call[\"id\"],\n",
        "        )\n",
        "    )\n",
        "    return {\"messages\": outputs}\n",
        "\n",
        "\n",
        "# Define the node that calls the model\n",
        "def call_model(\n",
        "    state: AgentState,\n",
        "    config: RunnableConfig,\n",
        "):\n",
        "    response = model.invoke([system_prompt] + state[\"messages\"], config)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the conditional edge that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we finish\n",
        "    if not last_message.tool_calls:\n",
        "        return \"end\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpuT5adUFrW6"
      },
      "source": [
        "### Xây dựng đồ thị"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2e-2beaBENNp"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Set the entrypoint as agent. This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use ‘agent‘.\n",
        "    # This means these are the edges taken after the ‘agent‘ node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\", # If ‘tools‘, then we call the tool node.\n",
        "        \"end\": END, # Otherwise we finish.\n",
        "    },\n",
        ")\n",
        "\n",
        "# We now add a normal edge from ‘tools‘ to ‘agent‘.\n",
        "# This means that after ‘tools‘ is called, ‘agent‘ node is called next.\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Now we can compile and visualize our graph\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "VOUmK-9LENLD",
        "outputId": "586bcb50-5de5-4bcc-8b66-8bb764c95df8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peoKzyTcGVac"
      },
      "source": [
        "### Kiểm tra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "syJWmZBxENIb",
        "outputId": "3dbabc79-6203-4c0c-b1f3-3e45fd362a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is the weather in vietnam? Then Triple it \n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-719527d5759f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what is the weather in vietnam? Then Triple it \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlast_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlast_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2431\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2434\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c109f61bdeb2>\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m ):\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# We return a list, because this will get added to the existing list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         return cast(\n\u001b[1;32m    369\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    946\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                 results.append(\n\u001b[0;32m--> 766\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    767\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1013\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\": [(\"user\", \"what is the weather in vietnam? Then Triple it \")]}\n",
        "\n",
        "for state in graph.stream(inputs, stream_mode=\"values\"):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    last_message.pretty_print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
